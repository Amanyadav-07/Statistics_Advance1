{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###1.  Explain the properties of the F-distribution."
      ],
      "metadata": {
        "id": "SZxVo33VCIk-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans:- The F-distribution is a continuous probability distribution that arises frequently in the context of statistical inference, particularly in analysis of variance (ANOVA) and regression analysis. Here are some key properties of the F-distribution:\n",
        "\n",
        "Definition: The F-distribution is defined as the ratio of two scaled chi-squared distributions. Specifically, if\n",
        "𝑋\n",
        "X and\n",
        "𝑌\n",
        "Y are independent chi-squared random variables with\n",
        "𝑘\n",
        "1\n",
        "k\n",
        "1\n",
        "​\n",
        "  and\n",
        "𝑘\n",
        "2\n",
        "k\n",
        "2\n",
        "​\n",
        "  degrees of freedom, respectively, then the ratio\n",
        "𝐹\n",
        "=\n",
        "𝑋\n",
        "/\n",
        "𝑘\n",
        "1\n",
        "𝑌\n",
        "/\n",
        "𝑘\n",
        "2\n",
        "F=\n",
        "Y/k\n",
        "2\n",
        "​\n",
        "\n",
        "X/k\n",
        "1\n",
        "​\n",
        "\n",
        "​\n",
        "  follows an F-distribution with\n",
        "𝑘\n",
        "1\n",
        "k\n",
        "1\n",
        "​\n",
        "  and\n",
        "𝑘\n",
        "2\n",
        "k\n",
        "2\n",
        "​\n",
        "  degrees of freedom.\n",
        "\n",
        "Shape: The F-distribution is positively skewed, meaning it has a long right tail. As the degrees of freedom increase, the distribution approaches a normal distribution.\n",
        "\n",
        "Degrees of Freedom: The F-distribution is characterized by two degrees of freedom:\n",
        "𝑘\n",
        "1\n",
        "k\n",
        "1\n",
        "​\n",
        "  (numerator) and\n",
        "𝑘\n",
        "2\n",
        "k\n",
        "2\n",
        "​\n",
        "  (denominator). These degrees of freedom affect the shape of the distribution.\n",
        "\n",
        "Mean and Variance:\n",
        "\n",
        "The mean of the F-distribution is given by\n",
        "𝑘\n",
        "2\n",
        "𝑘\n",
        "2\n",
        "−\n",
        "2\n",
        "k\n",
        "2\n",
        "​\n",
        " −2\n",
        "k\n",
        "2\n",
        "​\n",
        "\n",
        "​\n",
        "  for\n",
        "𝑘\n",
        "2\n",
        ">\n",
        "2\n",
        "k\n",
        "2\n",
        "​\n",
        " >2.\n",
        "The variance is given by\n",
        "2\n",
        "(\n",
        "𝑘\n",
        "1\n",
        ")\n",
        "(\n",
        "𝑘\n",
        "2\n",
        ")\n",
        "2\n",
        "(\n",
        "𝑘\n",
        "2\n",
        "+\n",
        "1\n",
        ")\n",
        "(\n",
        "𝑘\n",
        "2\n",
        "−\n",
        "2\n",
        ")\n",
        "2\n",
        "(\n",
        "𝑘\n",
        "2\n",
        "−\n",
        "4\n",
        ")\n",
        "(k\n",
        "2\n",
        "​\n",
        " −2)\n",
        "2\n",
        " (k\n",
        "2\n",
        "​\n",
        " −4)\n",
        "2(k\n",
        "1\n",
        "​\n",
        " )(k\n",
        "2\n",
        "​\n",
        " )\n",
        "2\n",
        " (k\n",
        "2\n",
        "​\n",
        " +1)\n",
        "​\n",
        "  for\n",
        "𝑘\n",
        "2\n",
        ">\n",
        "4\n",
        "k\n",
        "2\n",
        "​\n",
        " >4.\n",
        "Range: The F-distribution ranges from 0 to positive infinity. It cannot take negative values.\n",
        "\n",
        "Critical Values: Critical values of the F-distribution can be determined using F-distribution tables or statistical software, which are often used to test hypotheses about variances.\n",
        "\n",
        "Use in Hypothesis Testing: The F-distribution is commonly used in the context of ANOVA, where it helps to determine whether there are significant differences among group means. It's also used in testing the equality of variances.\n",
        "\n",
        "Relationship to Other Distributions: The F-distribution is closely related to the chi-squared distribution and the normal distribution. As mentioned, it can be derived from the ratio of chi-squared variables.\n",
        "\n",
        "Non-symmetry: Unlike the normal distribution, the F-distribution is not symmetric, which is important to consider when making statistical inferences."
      ],
      "metadata": {
        "id": "PWY_y4A6CLNu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?"
      ],
      "metadata": {
        "id": "JO7f_9aZCVI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans: - The F-distribution is commonly used in several types of statistical tests, primarily due to its properties relating to the comparison of variances and the testing of means. Here are some key areas where the F-distribution is applied:\n",
        "\n",
        "Analysis of Variance (ANOVA):\n",
        "\n",
        "Purpose: ANOVA is used to compare the means of three or more groups to determine if at least one group mean is significantly different from the others.\n",
        "Why F-distribution: The F-statistic is calculated as the ratio of the variance between the group means to the variance within the groups. Since variances are involved, the F-distribution is appropriate for testing the hypothesis about the equality of means.\n",
        "Regression Analysis:\n",
        "\n",
        "Purpose: In multiple regression, the F-test is used to assess the overall significance of the model.\n",
        "Why F-distribution: The F-statistic compares the model with predictors against a model with no predictors (intercept-only model). The distribution of the F-statistic under the null hypothesis follows an F-distribution, allowing researchers to test the significance of the model.\n",
        "Comparing Two Variances:\n",
        "\n",
        "Purpose: The F-test can be used to compare the variances of two independent samples to determine if they come from populations with equal variances.\n",
        "Why F-distribution: The ratio of the two sample variances follows an F-distribution, making it suitable for hypothesis testing regarding variances.\n",
        "Design of Experiments:\n",
        "\n",
        "Purpose: In experimental designs (like factorial designs), the F-test helps in analyzing the effects of different factors and their interactions.\n",
        "Why F-distribution: It assesses whether the variability due to treatments is significantly greater than the variability within treatments, again using the properties of the F-distribution.\n",
        "ANCOVA (Analysis of Covariance):\n",
        "\n",
        "Purpose: ANCOVA combines ANOVA and regression to compare means while controlling for one or more covariates.\n",
        "Why F-distribution: It utilizes the F-statistic to determine if the means of the dependent variable differ across groups after adjusting for covariates."
      ],
      "metadata": {
        "id": "2dEyj54-CepM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. What are the key assumptions required for conducting an F-test to compare the variances of two populations?"
      ],
      "metadata": {
        "id": "ZT7v_-19CkNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans: - When conducting an F-test to compare the variances of two populations, several key assumptions must be met to ensure the validity of the test results. Here are the main assumptions:\n",
        "\n",
        "Independence:\n",
        "\n",
        "The two samples must be independent of each other. This means that the selection or value of one sample does not influence the selection or value of the other sample.\n",
        "Normality:\n",
        "\n",
        "Both populations from which the samples are drawn should follow a normal distribution. While the F-test is somewhat robust to violations of this assumption with large sample sizes, significant departures from normality can affect the validity of the test, especially with smaller samples.\n",
        "Homogeneity of Variances:\n",
        "\n",
        "This assumption is implicit in the purpose of the F-test, which is to compare variances. The test is designed to determine if the variances are significantly different; hence, it assumes that the variances are similar under the null hypothesis.\n",
        "Random Sampling:\n",
        "\n",
        "The samples should be drawn randomly from their respective populations. This ensures that the samples are representative of the populations and helps to generalize the findings."
      ],
      "metadata": {
        "id": "5Xl8m2W7CuGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. What is the purpose of ANOVA, and how does it differ from a t-test?"
      ],
      "metadata": {
        "id": "TRYEYi67C1lr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans: - ANOVA (Analysis of Variance) and t-tests are both statistical methods used to compare means, but they serve different purposes and are suited for different situations. Here’s a breakdown of their purposes and key differences:\n",
        "\n",
        "Purpose of ANOVA\n",
        "Comparing Multiple Groups:\n",
        "\n",
        "The primary purpose of ANOVA is to compare the means of three or more groups to determine if there are any statistically significant differences among them.\n",
        "Assessing Variability:\n",
        "\n",
        "ANOVA evaluates the variation between group means relative to the variation within the groups. It helps to identify whether the variation observed is greater than what could be attributed to random sampling variability.\n",
        "Testing Null Hypothesis:\n",
        "\n",
        "The null hypothesis in ANOVA states that all group means are equal. If the null hypothesis is rejected, it suggests that at least one group mean is different from the others.\n",
        "Differences from t-tests\n",
        "Number of Groups:\n",
        "\n",
        "t-test: Typically used to compare the means of two groups (independent or paired).\n",
        "ANOVA: Used when comparing three or more groups.\n",
        "Types of Hypotheses:\n",
        "\n",
        "t-test: Tests the hypothesis that the means of two groups are equal.\n",
        "ANOVA: Tests the hypothesis that the means of multiple groups are equal; if significant, further tests (like post hoc tests) are often needed to identify which specific means differ.\n",
        "Variance Analysis:\n",
        "\n",
        "t-test: Focuses primarily on the difference between two means and assumes equal variances (for the independent t-test).\n",
        "ANOVA: Evaluates the overall variability among group means and considers both within-group and between-group variability.\n",
        "Output:\n",
        "\n",
        "t-test: Produces a t-statistic and a p-value indicating the significance of the difference between two means.\n",
        "ANOVA: Produces an F-statistic and a p-value, indicating whether there are significant differences among the means of multiple groups."
      ],
      "metadata": {
        "id": "6uMl2IdVDDNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more than two groups."
      ],
      "metadata": {
        "id": "N5rx29rZDGLY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans: - When to Use One-Way ANOVA\n",
        "Comparing More Than Two Groups:\n",
        "One-way ANOVA is appropriate when you want to compare the means of three or more groups. For example, if you have data from four different treatment groups and want to see if there are differences in outcomes.\n",
        "Why Use One-Way ANOVA Instead of Multiple t-tests\n",
        "Control of Type I Error Rate:\n",
        "\n",
        "When conducting multiple t-tests, the probability of committing a Type I error (incorrectly rejecting the null hypothesis) increases with each additional test. For example, if you perform three t-tests, the overall alpha level (the probability of making at least one Type I error) is higher than the nominal level (e.g., 0.05). ANOVA maintains the overall error rate, allowing for a single test with a controlled alpha level.\n",
        "Efficiency:\n",
        "\n",
        "Performing multiple t-tests can be time-consuming and cumbersome, especially with many groups. One-way ANOVA provides a single analysis that assesses all group differences simultaneously.\n",
        "Overall Comparison:\n",
        "\n",
        "ANOVA tests the null hypothesis that all group means are equal. If significant, you can follow up with post hoc tests to identify specific group differences, providing a clear approach to understanding the data.\n",
        "Statistical Power:\n",
        "\n",
        "One-way ANOVA is generally more powerful than multiple t-tests because it uses information from all groups in a single analysis rather than dividing the data into pairwise comparisons.\n",
        "Assumptions:\n",
        "\n",
        "One-way ANOVA assumes that the data are normally distributed and that variances are equal across groups (homogeneity of variance). While t-tests also have similar assumptions, ANOVA's framework allows for better handling of these assumptions through its design."
      ],
      "metadata": {
        "id": "pc9ftCnZDbcm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance. How does this partitioning contribute to the calculation of the F-statistic?"
      ],
      "metadata": {
        "id": "0h-uz8s_DfSc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Ans: - 1. Variance Partitioning\n",
        "Between-Group Variance (SSB):\n",
        "Definition: This component measures the variance due to the differences between the group means. It reflects how much the group means vary from the overall mean.\n",
        "Calculation: It is calculated as follows:\n",
        "𝑆\n",
        "𝑆\n",
        "𝐵\n",
        "=\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑘\n",
        "𝑛\n",
        "𝑖\n",
        "(\n",
        "𝑋\n",
        "ˉ\n",
        "𝑖\n",
        "−\n",
        "𝑋\n",
        "ˉ\n",
        ")\n",
        "2\n",
        "SSB=\n",
        "i=1\n",
        "∑\n",
        "k\n",
        "​\n",
        " n\n",
        "i\n",
        "​\n",
        " (\n",
        "X\n",
        "ˉ\n",
        "  \n",
        "i\n",
        "​\n",
        " −\n",
        "X\n",
        "ˉ\n",
        " )\n",
        "2\n",
        "\n",
        "where:\n",
        "𝑘\n",
        "k = number of groups,\n",
        "𝑛\n",
        "𝑖\n",
        "n\n",
        "i\n",
        "​\n",
        "  = number of observations in group\n",
        "𝑖\n",
        "i,\n",
        "𝑋\n",
        "ˉ\n",
        "𝑖\n",
        "X\n",
        "ˉ\n",
        "  \n",
        "i\n",
        "​\n",
        "  = mean of group\n",
        "𝑖\n",
        "i,\n",
        "𝑋\n",
        "ˉ\n",
        "X\n",
        "ˉ\n",
        "  = overall mean of all groups.\n",
        "Within-Group Variance (SSW):\n",
        "Definition: This component measures the variance within each group. It reflects how much individual observations within each group vary around their respective group means.\n",
        "Calculation: It is calculated as follows:\n",
        "𝑆\n",
        "𝑆\n",
        "𝑊\n",
        "=\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑘\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝑖\n",
        "(\n",
        "𝑋\n",
        "𝑖\n",
        "𝑗\n",
        "−\n",
        "𝑋\n",
        "ˉ\n",
        "𝑖\n",
        ")\n",
        "2\n",
        "SSW=\n",
        "i=1\n",
        "∑\n",
        "k\n",
        "​\n",
        "  \n",
        "j=1\n",
        "∑\n",
        "n\n",
        "i\n",
        "​\n",
        "\n",
        "​\n",
        " (X\n",
        "ij\n",
        "​\n",
        " −\n",
        "X\n",
        "ˉ\n",
        "  \n",
        "i\n",
        "​\n",
        " )\n",
        "2\n",
        "\n",
        "where:\n",
        "𝑋\n",
        "𝑖\n",
        "𝑗\n",
        "X\n",
        "ij\n",
        "​\n",
        "  = individual observation\n",
        "𝑗\n",
        "j in group\n",
        "𝑖\n",
        "i.\n",
        "2. Total Variance\n",
        "The total variance in the data (SST) can be expressed as the sum of the between-group variance and the within-group variance:\n",
        "\n",
        "𝑆\n",
        "𝑆\n",
        "𝑇\n",
        "=\n",
        "𝑆\n",
        "𝑆\n",
        "𝐵\n",
        "+\n",
        "𝑆\n",
        "𝑆\n",
        "𝑊\n",
        "SST=SSB+SSW\n",
        "3. Contribution to the F-statistic\n",
        "The F-statistic is calculated using the means of the squared variances derived from the above partitioning. Here’s how it works:\n",
        "\n",
        "Mean Square Between (MSB): This is the average between-group variance, calculated as:\n",
        "\n",
        "𝑀\n",
        "𝑆\n",
        "𝐵\n",
        "=\n",
        "𝑆\n",
        "𝑆\n",
        "𝐵\n",
        "𝑘\n",
        "−\n",
        "1\n",
        "MSB=\n",
        "k−1\n",
        "SSB\n",
        "​\n",
        "\n",
        "where\n",
        "𝑘\n",
        "−\n",
        "1\n",
        "k−1 represents the degrees of freedom associated with the between-group variance.\n",
        "\n",
        "Mean Square Within (MSW): This is the average within-group variance, calculated as:\n",
        "\n",
        "𝑀\n",
        "𝑆\n",
        "𝑊\n",
        "=\n",
        "𝑆\n",
        "𝑆\n",
        "𝑊\n",
        "𝑁\n",
        "−\n",
        "𝑘\n",
        "MSW=\n",
        "N−k\n",
        "SSW\n",
        "​\n",
        "\n",
        "where\n",
        "𝑁\n",
        "N is the total number of observations and\n",
        "𝑁\n",
        "−\n",
        "𝑘\n",
        "N−k represents the degrees of freedom associated with the within-group variance.\n",
        "\n",
        "F-statistic: The F-statistic is then calculated as the ratio of these two mean squares:\n",
        "\n",
        "𝐹\n",
        "=\n",
        "𝑀\n",
        "𝑆\n",
        "𝐵\n",
        "𝑀\n",
        "𝑆\n",
        "𝑊\n",
        "F=\n",
        "MSW\n",
        "MSB\n",
        "​\n",
        "\n",
        "4. Interpretation\n",
        "A large F-statistic suggests that the between-group variance is significantly larger than the within-group variance, indicating that at least one group mean is different from the others.\n",
        "Conversely, a small F-statistic implies that any observed differences in group means are likely due to random sampling variation rather than systematic differences."
      ],
      "metadata": {
        "id": "uCfl2qt4DqaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?"
      ],
      "metadata": {
        "id": "RqgZFI_BD167"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans: - 1. Handling Uncertainty\n",
        "Classical Approach:\n",
        "Probability as Long-Run Frequency: In the frequentist framework, probability is interpreted as the long-run frequency of events. Uncertainty is quantified through confidence intervals and p-values, which provide information about how likely the observed data is under a null hypothesis.\n",
        "Hypothesis Testing: The frequentist approach focuses on rejecting or not rejecting the null hypothesis based on p-values derived from the F-statistic.\n",
        "Bayesian Approach:\n",
        "Probability as Belief: In the Bayesian framework, probability represents a degree of belief or certainty about an event. Uncertainty is modeled using probability distributions, and prior beliefs can be incorporated.\n",
        "Credible Intervals: Bayesian methods produce credible intervals that provide a range of values for parameters, reflecting uncertainty directly in terms of probability (e.g., there’s a 95% probability that the true parameter lies within this interval).\n",
        "2. Parameter Estimation\n",
        "Classical Approach:\n",
        "Point Estimates: Frequentist ANOVA provides point estimates for group means and variances, often accompanied by confidence intervals.\n",
        "Assumption-Driven: Parameter estimation relies heavily on assumptions about the underlying distribution of the data (e.g., normality, homogeneity of variances).\n",
        "Bayesian Approach:\n",
        "Posterior Distributions: Bayesian ANOVA yields full posterior distributions for parameters, allowing for more comprehensive insights into uncertainty around estimates.\n",
        "Incorporation of Priors: Bayesian methods allow the inclusion of prior information or beliefs about parameters, which can lead to different estimates depending on the chosen priors.\n",
        "3. Hypothesis Testing\n",
        "Classical Approach:\n",
        "Null Hypothesis Significance Testing (NHST): Frequentist ANOVA tests a null hypothesis (e.g., all group means are equal) against an alternative hypothesis. Decisions are made based on p-values, with a common threshold (e.g., α = 0.05) determining whether to reject the null hypothesis.\n",
        "Frequentist Limitations: The frequentist approach does not provide a direct probability of the null hypothesis being true; instead, it evaluates the data under the assumption that the null hypothesis is true.\n",
        "Bayesian Approach:\n",
        "Bayes Factors: Bayesian hypothesis testing can involve Bayes factors, which provide a ratio of the probabilities of the data under different hypotheses. This approach quantifies the evidence for one hypothesis relative to another.\n",
        "Direct Probability Statements: Bayesian methods allow for direct probability statements about hypotheses, making it possible to express how likely a hypothesis is given the observed data."
      ],
      "metadata": {
        "id": "999C0HCzD8Q4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###8. Question: You have two sets of data representing the incomes of two different professions1\n",
        "V Profession A: [48, 52, 55, 60, 62'\n",
        "V Profession B: [45, 50, 55, 52, 47]"
      ],
      "metadata": {
        "id": "DGIzn1ePEJWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Data for the two professions\n",
        "profession_a = np.array([48, 52, 55, 60, 62])\n",
        "profession_b = np.array([45, 50, 55, 52, 47])\n",
        "\n",
        "# Calculate the variances\n",
        "var_a = np.var(profession_a, ddof=1)  # Sample variance\n",
        "var_b = np.var(profession_b, ddof=1)  # Sample variance\n",
        "\n",
        "# Calculate the F-statistic\n",
        "f_statistic = var_a / var_b\n",
        "\n",
        "# Degrees of freedom\n",
        "df_a = len(profession_a) - 1\n",
        "df_b = len(profession_b) - 1\n",
        "\n",
        "# Calculate the p-value\n",
        "p_value = 1 - stats.f.cdf(f_statistic, df_a, df_b)\n",
        "\n",
        "# Output the results\n",
        "print(f\"F-statistic: {f_statistic}\")\n",
        "print(f\"p-value: {p_value}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGmChFXFEb6q",
        "outputId": "066cd3b6-9c4e-4315-d9be-b5655ca8aa98"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 2.089171974522293\n",
            "p-value: 0.24652429950266952\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###9. Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in average heights between three different regions with the following data1\n",
        "V Region A: [160, 162, 165, 158, 164']\n",
        "V Region B: [172, 175, 170, 168, 174']\n",
        "V Region C: [180, 182, 179, 185, 183']"
      ],
      "metadata": {
        "id": "mB9OZuBkEg_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Data for the three regions\n",
        "region_a = np.array([160, 162, 165, 158, 164])\n",
        "region_b = np.array([172, 175, 170, 168, 174])\n",
        "region_c = np.array([180, 182, 179, 185, 183])\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "f_statistic, p_value = stats.f_oneway(region_a, region_b, region_c)\n",
        "\n",
        "# Output the results\n",
        "print(f\"F-statistic: {f_statistic}\")\n",
        "print(f\"p-value: {p_value}\")\n",
        "\n",
        "# Interpretation\n",
        "alpha = 0.05  # significance level\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: There are significant differences in average heights between regions.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: No significant differences in average heights between regions.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RO1SIQSjEwzW",
        "outputId": "843a2e1d-9c08-46ec-8e54-9a56979a42cb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 67.87330316742101\n",
            "p-value: 2.870664187937026e-07\n",
            "Reject the null hypothesis: There are significant differences in average heights between regions.\n"
          ]
        }
      ]
    }
  ]
}